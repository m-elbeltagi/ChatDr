{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba35b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import sys \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc55c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# setting device as GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device in use: {} \\n'.format(device))\n",
    "\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "BATCH_SIZE = 24\n",
    "N_EPOCHS = 20\n",
    "LOGGING_STEPS = 100 # roughly every 1/5 epoch (for this batch size)\n",
    "WARMUP_STEPS = 500 # roughly 1 epoch of warmup (for this batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ead025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'.\\SciQ\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921572cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>mesophilic organisms</td>\n",
       "      <td>Mesophiles grow best in moderate temperature, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>coriolis effect</td>\n",
       "      <td>Without Coriolis Effect the global winds would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Summary Changes of state are examples of phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>alpha decay</td>\n",
       "      <td>All radioactive decay is dangerous to living t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Example 3.5 Calculating Projectile Motion: Hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                answer  \\\n",
       "0  What type of organism is commonly used in prep...  mesophilic organisms   \n",
       "1  What phenomenon makes global winds blow northe...       coriolis effect   \n",
       "2  Changes from a less-ordered state to a more-or...            exothermic   \n",
       "3     What is the least dangerous radioactive decay?           alpha decay   \n",
       "4  Kilauea in hawaii is the world’s most continuo...         smoke and ash   \n",
       "\n",
       "                                             context  \n",
       "0  Mesophiles grow best in moderate temperature, ...  \n",
       "1  Without Coriolis Effect the global winds would...  \n",
       "2  Summary Changes of state are examples of phase...  \n",
       "3  All radioactive decay is dangerous to living t...  \n",
       "4  Example 3.5 Calculating Projectile Motion: Hot...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['distractor1','distractor2','distractor3'], axis=1)\n",
    "data.columns = ['question', 'answer', 'context']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83bcfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What type of organism is commonly used in prep...</td>\n",
       "      <td>mesophilic organisms</td>\n",
       "      <td>Mesophiles grow best in moderate temperature, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What phenomenon makes global winds blow northe...</td>\n",
       "      <td>coriolis effect</td>\n",
       "      <td>Without Coriolis Effect the global winds would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Changes from a less-ordered state to a more-or...</td>\n",
       "      <td>exothermic</td>\n",
       "      <td>Summary Changes of state are examples of phase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the least dangerous radioactive decay?</td>\n",
       "      <td>alpha decay</td>\n",
       "      <td>All radioactive decay is dangerous to living t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kilauea in hawaii is the world’s most continuo...</td>\n",
       "      <td>smoke and ash</td>\n",
       "      <td>Example 3.5 Calculating Projectile Motion: Hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question                answer  \\\n",
       "0  What type of organism is commonly used in prep...  mesophilic organisms   \n",
       "1  What phenomenon makes global winds blow northe...       coriolis effect   \n",
       "2  Changes from a less-ordered state to a more-or...            exothermic   \n",
       "3     What is the least dangerous radioactive decay?           alpha decay   \n",
       "4  Kilauea in hawaii is the world’s most continuo...         smoke and ash   \n",
       "\n",
       "                                             context  \n",
       "0  Mesophiles grow best in moderate temperature, ...  \n",
       "1  Without Coriolis Effect the global winds would...  \n",
       "2  Summary Changes of state are examples of phase...  \n",
       "3  All radioactive decay is dangerous to living t...  \n",
       "4  Example 3.5 Calculating Projectile Motion: Hot...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dropping all rows with Null values (only context columns have null values in this case)\n",
    "print (data['context'].isnull().sum())\n",
    "data = data.dropna()\n",
    "\n",
    "## resetting index because deleting rows doesn't re-assing the indices\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ca73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_answer_start(batch):\n",
    "    ''' The data is missing the indices of the location \n",
    "    of the start of the answer inside the context,\n",
    "    we find those here (this function takes a pandas dataframe\n",
    "    and modifies the answer column to include this as a dict),\n",
    "    later they can be used to find the end positions \n",
    "    (after splitting contexts below if necessary),\n",
    "    then positions labels for prediction,\n",
    "    need to lower case the context and answers first,\n",
    "    because find() is case-senstitive'''\n",
    "    \n",
    "    def start_position(context, answer):\n",
    "        pos = context.lower().find(answer.lower())\n",
    "        \n",
    "        ## find returns -1 if doesn't find\n",
    "        if pos == -1:\n",
    "            return {'text':[answer], 'answer_start':[0]}\n",
    "        else:\n",
    "            return {'text':[answer], 'answer_start':[pos]}\n",
    "    \n",
    "    batch['answer'] = batch.apply(lambda row : start_position(row['context'], row['answer']), axis = 1)\n",
    "    \n",
    "    return batch\n",
    "\n",
    "data = add_answer_start(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66bb5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of tokens in these contexts is: 814\n",
      "This exceeds tha model max length of 512\n"
     ]
    }
   ],
   "source": [
    "## load pretrained model and it's tokenizer\n",
    "MODEL_NAME = 'distilbert-base-cased-distilled-squad'\n",
    "# MODEL_NAME = 'deepset/minilm-uncased-squad2'\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LENGTH = tokenizer.model_max_length\n",
    "\n",
    "def check_context_lengths(batch):\n",
    "    '''function to check if any context length (by counting tokens) \n",
    "    exceeds the max context length the model accepts'''\n",
    "\n",
    "    encoded_data = tokenizer(list(batch['question']), list(batch['context']), truncation=False, padding=\"max_length\")\n",
    "    max_context_length = max([len(id) for id in encoded_data['input_ids']])\n",
    "    print('max number of tokens in these contexts is: {}'.format(max_context_length))\n",
    "    if max_context_length > MAX_LENGTH:\n",
    "        print ('This exceeds tha model max length of {}'.format(MAX_LENGTH))\n",
    "        return True\n",
    "    else:\n",
    "        print (\"This doesn't exceed max model length of {}\".format(MAX_LENGTH))\n",
    "        return False\n",
    "\n",
    "exceeds_model_length = check_context_lengths(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f8f8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    '''Tokenize the context and question,\n",
    "    becasue some context sizes exceed the model max length \n",
    "    (with trunction set to apply only to the second entry aka contexts)\n",
    "    we will set a maximum then use a sliding sindow (stride) to guarantee\n",
    "    that the cutoff will never cut the answer, and we use the same question\n",
    "    for all the new contexts, only now some of them may not contain the answer,\n",
    "    for those we set start & end positions to be 0 (aka the [CLS] token)\n",
    "    return_overflowing_tokens so we get those overflowing tokens\n",
    "    and return_offsets_mapping so we can use those to find the\n",
    "    answer end position (provided we know the start position).\n",
    "    \n",
    "    In the case that none of the xontexts exceed model length we use a \n",
    "    simpler more familiar tokenizatin scheme'''\n",
    "    \n",
    "#     if exceeds_model_length:\n",
    "#         return tokenizer(batch['question'], batch['context'], truncation='only_second', max_length = MAX_LENGTH,\n",
    "#                               stride=int(MAX_LENGTH/2), return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "#     elif not exceeds_model_length:\n",
    "#         return tokenizer(batch['question'], batch['context'], truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c56ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch):\n",
    "    '''This function takes the data in a (HF) dataset,\n",
    "    applies the tokenize function, then finds the start and end\n",
    "    positions (treating the split up contexts case, using the \n",
    "    overflow_to_sample_mapping to find the original example\n",
    "    (or as theey call it \"sample\" that the split up context came from)'''\n",
    "    \n",
    "    '''for the tokenizer: tokenize the context and question,\n",
    "    becasue some context sizes exceed the model max length \n",
    "    (with trunction set to apply only to the second entry aka contexts)\n",
    "    we will set a maximum then use a sliding sindow (stride) to guarantee\n",
    "    that the cutoff will never cut the answer, and we use the same question\n",
    "    for all the new contexts, only now some of them may not contain the answer,\n",
    "    for those we set start & end positions to be 0 (aka the [CLS] token)\n",
    "    return_overflowing_tokens so we get those overflowing tokens\n",
    "    and return_offsets_mapping so we can use those to find the\n",
    "    answer end position (provided we know the start position).\n",
    "    \n",
    "    In the case that none of the xontexts exceed model length we use a \n",
    "    simpler more familiar tokenizatin scheme'''\n",
    "    \n",
    "    \n",
    "    ## removing whitespace before and after the question\n",
    "    questions = [q.strip() for q in batch['question']]\n",
    "    \n",
    "    ## I Call it \"tokenized\" here, but equivalently \"encoded\", which is what I use below\n",
    "    if exceeds_model_length:\n",
    "        tokenized_batch = tokenizer(batch['question'], batch['context'], truncation='only_second', \n",
    "                                      max_length = MAX_LENGTH, stride=int(MAX_LENGTH/2), \n",
    "                                      return_overflowing_tokens=True, return_offsets_mapping=True)\n",
    "    elif not exceeds_model_length:\n",
    "        tokenized_batch = tokenizer(batch['question'], batch['context'],\n",
    "                                      truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    offset_mapping = tokenized_batch.pop('offset_mapping')\n",
    "    sample_map = tokenized_batch.pop('overflow_to_sample_mapping')\n",
    "    answers = batch['answer']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    \n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
    "        sequence_ids = tokenized_batch.sequence_ids(i)\n",
    "\n",
    "        '''Find the start and end of the context, \n",
    "        the input_ids contain the tokenized question and context,\n",
    "        and (THIS IS MODEL DEPENDENT) the context has id 1 in\n",
    "        the sequence ids''' \n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    tokenized_batch['start_positions'] = start_positions\n",
    "    tokenized_batch['end_positions'] = end_positions\n",
    "    \n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c637847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_decorator(preprocess_function, dataframe):\n",
    "    '''This function takes the data in a pandas dataframe, \n",
    "    converts it to a (HF) dataset (this step so we don't have \n",
    "    to worry about the list input to tokenizer and so on),\n",
    "    removes the OG columns because they have different size\n",
    "    (after splitting some examples), so there isnt a mismatch\n",
    "    in the final dataset'''\n",
    "    \n",
    "\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    encoded = dataset.map(preprocess_function, batched=True, batch_size=BATCH_SIZE, \n",
    "                remove_columns=dataset.column_names)\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "\n",
    "encoded_dataset = preprocess_decorator(preprocess_data, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb5617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 10586\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print (encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77892b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(encoded_full_data):\n",
    "    train_dataset = encoded_full_data.select(range(int(TRAIN_PERCENTAGE*len(encoded_full_data))))\n",
    "    test_dataset = encoded_full_data.select(range(int(TRAIN_PERCENTAGE*len(encoded_full_data)), len(encoded_full_data)))\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_data(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4ac1c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we structured this like the SQuAD dataset so we can use it's metric\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    '''This function only works after training is concluded, cant use it in the middle of training'''\n",
    "    \n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answer\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5d7fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=r'.\\distilbertSQUAD_finteuned_SciQ', \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps',\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=LOGGING_STEPS,\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS, # no. of warmup steps till it reaches set value for learning rate (default strat: linear)\n",
    "    fp16=True,\n",
    "    report_to='tensorboard')\n",
    "\n",
    "'''no need for data collator, becausesamples are padded to max length.\n",
    "    I disabled early stopping to try to reach the interploation regime (after overfitting regime)'''\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ffe43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\Miniconda3\\envs\\spyder-env\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 9527\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7940\n",
      "  Number of trainable parameters = 64799234\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='7940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/7940 04:42 < 32:43, 3.53 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.681300</td>\n",
       "      <td>1.422232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.223200</td>\n",
       "      <td>1.125617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.082100</td>\n",
       "      <td>1.001087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.967700</td>\n",
       "      <td>0.936279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.862900</td>\n",
       "      <td>0.880835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>0.882369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.772400</td>\n",
       "      <td>0.859374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.780500</td>\n",
       "      <td>0.879810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.877670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>0.892970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-100\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-100\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-100\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-200\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-200\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-200\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-300\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-300\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-300\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-400\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-400\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-400\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-500\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-500\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-600\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-600\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-600\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-800\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-800\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-800\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-900\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-900\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-900\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-900\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-900\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1059\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-1000\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-1000\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-1000\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from .\\distilbertSQUAD_finteuned_SciQ\\checkpoint-700 (score: 0.8593740463256836).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.9266230850219727, metrics={'train_runtime': 283.9749, 'train_samples_per_second': 670.975, 'train_steps_per_second': 27.96, 'total_flos': 2490839087742096.0, 'train_loss': 0.9266230850219727, 'epoch': 2.52})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f15d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to .\\distilbertSQUAD_finteuned_SciQ\n",
      "Configuration saved in .\\distilbertSQUAD_finteuned_SciQ\\config.json\n",
      "Model weights saved in .\\distilbertSQUAD_finteuned_SciQ\\pytorch_model.bin\n",
      "tokenizer config file saved in .\\distilbertSQUAD_finteuned_SciQ\\tokenizer_config.json\n",
      "Special tokens file saved in .\\distilbertSQUAD_finteuned_SciQ\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
