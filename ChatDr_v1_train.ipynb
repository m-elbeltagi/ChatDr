{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bedeea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use: cuda \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelWithLMHead, GPT2LMHeadModel\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "import sys \n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "save_path = r'C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr'\n",
    "contexts_path = r'C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\contexts'\n",
    "\n",
    "# setting device as GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device in use: {} \\n'.format(device))\n",
    "\n",
    "## the model we are using (from Hugging Face)\n",
    "MODEL_NAME = 'gpt2'\n",
    "\n",
    "## the percentage of full dataset to be used for training\n",
    "TRAIN_PERCENTAGE = 0.9\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_EPOCHS = 100\n",
    "LOGGING_STEPS = 500\n",
    "WARMUP_STEPS = 3000 ##roughly 3 epochs of warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638515c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_contexts():\n",
    "    ''' This finction joins the contexts \n",
    "    from separate files into one array\n",
    "    '''\n",
    "    \n",
    "    contexts = []\n",
    "    files = os.listdir(contexts_path)\n",
    "    for file in files:\n",
    "        current_path = (contexts_path + r'\\{}'.format(file))\n",
    "        current_text = open(current_path, 'r', encoding='utf8').read()\n",
    "        contexts.append(current_text)\n",
    "    \n",
    "    return contexts\n",
    "\n",
    "contexts = join_contexts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdaab60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing tokenizer and model (with language modeling head) and setting context length\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "transformer = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(device)\n",
    "max_length = tokenizer.model_max_length\n",
    "\n",
    "## padding token required when tokenizing the text below, but apparently gpt-2 tokenizer doesn't have it?\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "## resizing becasue we added an extra token\n",
    "transformer.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9474b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d0534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['input_ids'], padding='max_length', truncation=True, max_length=max_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a79bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def return_HF_datasets(contexts_array):\n",
    "    ''' This function will take in an array (python list) containg all \n",
    "    the contexts, tokenize it, and return the tokenized train and test\n",
    "    Hugging Face datasets\n",
    "    '''\n",
    "    df = pd.DataFrame(contexts_array)\n",
    "    df.columns = ['input_ids']   # gpt2LMhead expects it to be called this, tokenizer should already name it that?\n",
    "    full_dataset = Dataset.from_pandas(df)\n",
    "    tokenized_dataset = full_dataset.map(tokenize, batched=True, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    ## splitting into test and train\n",
    "    train_dataset = tokenized_dataset.select(range(int(TRAIN_PERCENTAGE*len(full_dataset))))\n",
    "    test_dataset = tokenized_dataset.select(range(int(TRAIN_PERCENTAGE*len(full_dataset)), len(full_dataset)))\n",
    "                                         \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = return_HF_datasets(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ba9207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path + r'\\gpt2_finetune_ckpts', \n",
    "    overwrite_output_dir=True, \n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    "    per_device_eval_batch_size=BATCH_SIZE, \n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy='steps',\n",
    "    disable_tqdm=False,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=LOGGING_STEPS,\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS, # no. of warmup steps till it reaches set value for learning rate (default srat: linear)\n",
    "    fp16=True,\n",
    "    report_to='tensorboard')\n",
    "    \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=transformer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfd4719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M\\Miniconda3\\envs\\spyder-env\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 936\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 93600\n",
      "  Number of trainable parameters = 124440576\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16000' max='93600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16000/93600 1:01:44 < 4:59:31, 4.32 it/s, Epoch 17/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>23.700200</td>\n",
       "      <td>5.026304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.575200</td>\n",
       "      <td>3.431267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.233200</td>\n",
       "      <td>3.105590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.035000</td>\n",
       "      <td>2.922549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.847300</td>\n",
       "      <td>2.822305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.718900</td>\n",
       "      <td>2.753305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.607200</td>\n",
       "      <td>2.687626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.528300</td>\n",
       "      <td>2.656788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.421000</td>\n",
       "      <td>2.592282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.343500</td>\n",
       "      <td>2.574138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.284000</td>\n",
       "      <td>2.536023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.221200</td>\n",
       "      <td>2.528711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.140900</td>\n",
       "      <td>2.483347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.064600</td>\n",
       "      <td>2.476877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.054600</td>\n",
       "      <td>2.481242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.974700</td>\n",
       "      <td>2.441262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.936500</td>\n",
       "      <td>2.469816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.882900</td>\n",
       "      <td>2.477725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.865100</td>\n",
       "      <td>2.428919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.788100</td>\n",
       "      <td>2.409986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.763800</td>\n",
       "      <td>2.401652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.731800</td>\n",
       "      <td>2.418499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.640000</td>\n",
       "      <td>2.420393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.683900</td>\n",
       "      <td>2.416975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.604700</td>\n",
       "      <td>2.384856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.580900</td>\n",
       "      <td>2.382164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.550600</td>\n",
       "      <td>2.366480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.513500</td>\n",
       "      <td>2.398364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.452500</td>\n",
       "      <td>2.382241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.480600</td>\n",
       "      <td>2.397330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.397000</td>\n",
       "      <td>2.376385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.413600</td>\n",
       "      <td>2.413001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-3500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-4500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-5500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-6500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-7500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-8500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-9500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-10500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-11500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-12500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-14500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15500\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15500\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-15500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 104\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-16000\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-16000\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-16000\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best model from C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\checkpoint-13500 (score: 2.3664803504943848).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16000, training_loss=2.7510974502563474, metrics={'train_runtime': 3706.4032, 'train_samples_per_second': 25.254, 'train_steps_per_second': 25.254, 'total_flos': 8361345024000000.0, 'train_loss': 2.7510974502563474, 'epoch': 17.09})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649d682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\n",
      "Configuration saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\config.json\n",
      "Model weights saved in C:\\Users\\M\\OneDrive - Carleton University\\Documents\\my_stuff\\Projects\\chatDr\\gpt2_finetune_ckpts\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b13673e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatDr = pipeline('text-generation', model=(r'.\\gpt2_finetune_ckpts'), \n",
    "                  tokenizer=tokenizer, device=torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6541a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Inguinal Hernia Repair\"\n",
    "outputs = chatDr(prompt, max_length = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65fdfc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inguinal Hernia Repair, PREOPERATIVE DIAGNOSES,1. Left inguinal hernia.,2. Right inguinal hernia.,POSTOPERATIVE DIAGNOSES,1. Left inguinal hernia.,2. Right inguinal hernia.,OPERATION,2. Injection of Tegaderm.,PROCEDURE,1. Left inguinal hernia.,2. Irrigation and debridement of right inguinal hernia.,ANESTHESIA:,Local MAC.,ESTIMATED BLOOD LOSS:, Less than 50 mL.,GROSS OPERATIVE FINDINGS,Following the administration of sedation and local MAC anesthesia, a longitudinal incision was made in the left inguinal pillar below the level of the hernia sac. The hernia sac was opened up and the incision was dissected out laterally. The luminescent probe was easily able to be inserted into the hernia sac and the hernias were noted to be inflamed. The intraocular lens was inspected. The hernia sac was well perfused. The hernia sac was infiltrated with 0.25% Marcaine without epinephrine. We\n"
     ]
    }
   ],
   "source": [
    "print (outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d184a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
